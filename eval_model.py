"""
eval_model.py - æ¨¡å‹è¯„ä¼°è„šæœ¬
åŠŸèƒ½ï¼š
1. åŠ è½½è®­ç»ƒå¥½çš„LSTMæ¨¡å‹
2. è¯„ä¼°æ–°æ•°æ®é›†
3. è¾“å‡ºå‡†ç¡®ç‡/F1åˆ†æ•°/é«˜è¯¯å·®æ ·æœ¬
"""

import os
import json
import torch
import torch.nn as nn
import numpy as np
from collections import defaultdict
from torch.utils.data import DataLoader
from torch.nn.utils.rnn import pad_sequence
from models.LSTM import LSTMWithAttention
from consts import difficulty_mapping, max_tags, special_indices
from sklearn.metrics import f1_score

from utils.chartDataset import ChartDataset

# é…ç½®å‚æ•°
MODEL_PATH = "trained_models/7t82x81z/best_model.pth"  # æ¨¡å‹è·¯å¾„
DATA_PATH = "./new_data"  # æ–°æ•°æ®è·¯å¾„
BATCH_SIZE = 32
THRESHOLD = 0.5


def load_model():
    """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
    model = LSTMWithAttention(
        input_size=18,
        hidden_size=128,
        num_layers=2,
        output_size=max_tags + 1,
        special_indices=special_indices,
        special_weight=10.0,
    ).cuda()

    if os.path.exists(MODEL_PATH):
        model.load_state_dict(torch.load(MODEL_PATH))
        print(f"âœ… æˆåŠŸåŠ è½½æ¨¡å‹: {MODEL_PATH}")
    else:
        raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ {MODEL_PATH} æœªæ‰¾åˆ°")

    model.eval()
    return model


def process_new_data():
    """å¤„ç†æ–°æ•°æ®"""
    # å‡è®¾æ–°æ•°æ®æ ¼å¼ä¸è®­ç»ƒæ•°æ®ç›¸åŒ
    json_data_list = []

    # è¯»å–æ–°æ•°æ®
    for filename in os.listdir(DATA_PATH):
        if filename.endswith(".json"):
            with open(os.path.join(DATA_PATH, filename), "r") as f:
                json_data = json.load(f)
                filename = filename.replace(".json", "")
                song_id, level = filename.split("_")
                json_data_list.append(
                    {
                        "song_id": song_id,
                        "level_index": int(level),
                        "data": json_data,
                        "keywords": [],
                    }
                )

    print(f"ğŸ“ åŠ è½½äº† {len(json_data_list)} ä¸ªæ–°æ•°æ®æ ·æœ¬")
    return json_data_list

fixed_note_length = 2000

def collate_fn(batch):
    data, labels, metadata = zip(*batch)

    # å¤„ç† data
    data_padded = pad_sequence(data, batch_first=True)
    if data_padded.size(1) > fixed_note_length:
        data_padded = data_padded[:, :fixed_note_length, :]
    else:
        padding_size = fixed_note_length - data_padded.size(1)
        data_padded = torch.nn.functional.pad(data_padded, (0, 0, 0, padding_size))

    # å¤„ç† labels
    labels_padded = torch.stack(labels)

    return data_padded, labels_padded, metadata

def evaluate(model, data_loader):
    """è¯„ä¼°æ¨¡å‹è¡¨ç°"""
    all_probs = []
    all_preds = []
    all_labels = []
    all_metadata = []

    with torch.no_grad():
        for inputs, labels, metadata in data_loader:
            inputs = inputs.cuda()
            outputs, _ = model(inputs)

            probs = torch.sigmoid(outputs).cpu()
            # æ‰¾å‡º probs top5 çš„ç´¢å¼•
            preds = probs.topk(10)[1]
            # preds = (probs >= THRESHOLD).float()

            all_probs.extend(probs.numpy())
            all_preds.extend(preds.numpy())
            if labels is not None:
                all_labels.extend(labels.numpy())
            all_metadata.extend(metadata)

    return np.array(all_probs), np.array(all_preds), np.array(all_labels), all_metadata


def calculate_metrics(probs, labels):
    """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
    preds = (probs >= THRESHOLD).astype(int)
    flat_preds = preds.flatten()
    flat_labels = labels.flatten()

    accuracy = np.mean(flat_preds == flat_labels)
    f1 = f1_score(flat_labels, flat_preds, average="micro")

    return {"accuracy": accuracy, "f1_score": f1}


def find_high_error_samples(probs, labels, metadata, top_n=5):
    """æŸ¥æ‰¾é«˜è¯¯å·®æ ·æœ¬"""
    losses = -np.mean(
        labels * np.log(probs + 1e-8) + (1 - labels) * np.log(1 - probs + 1e-8), axis=1
    )

    high_error_indices = np.argsort(losses)[-top_n:][::-1]
    high_error_samples = []

    for idx in high_error_indices:
        high_error_samples.append(
            {
                "metadata": metadata[idx],
                "loss": losses[idx],
                "prediction": probs[idx].tolist(),
                "label": labels[idx].tolist(),
            }
        )

    return high_error_samples


def main():
    """ä¸»å‡½æ•°"""
    # 1. åŠ è½½æ¨¡å‹
    model = load_model()

    # 2. å¤„ç†æ–°æ•°æ®
    data_list = process_new_data()
    note_dataset = ChartDataset(data_list)

    # 3. åˆ›å»ºæ•°æ®åŠ è½½å™¨
    test_loader = DataLoader(
        note_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn
    )

    # 4. è¿›è¡Œè¯„ä¼°
    probs, preds, labels, metadata = evaluate(model, test_loader)
    print(f"é¢„æµ‹ç»“æœï¼š{preds}")

    # 5. è®¡ç®—æŒ‡æ ‡
    metrics = calculate_metrics(probs, labels)
    print("\nğŸ“Š è¯„ä¼°ç»“æœ:")
    print(f"å‡†ç¡®ç‡: {metrics['accuracy']:.4f}")
    print(f"F1åˆ†æ•°: {metrics['f1_score']:.4f}")

    # 6. åˆ†æé«˜è¯¯å·®æ ·æœ¬
    high_error_samples = find_high_error_samples(probs, labels, metadata)
    print("\nâš ï¸ é«˜è¯¯å·®æ ·æœ¬:")
    for sample in high_error_samples:
        print(f"ID: {sample['metadata']['song_id']}")
        print(f"Loss: {sample['loss']:.4f}")
        print(f"é¢„æµ‹: {sample['prediction']}")
        print(f"æ ‡ç­¾: {sample['label']}\n")


if __name__ == "__main__":
    main()
